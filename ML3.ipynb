{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM37L5G+ngKMX4zPItN08Tc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirlevy1170/ML_homework/blob/main/ML3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise we will optimize the log loss defined\n",
        "as follows:\n",
        "\n",
        "ℓlog (w, x, y) = log\n",
        "1 + e\n",
        "−yw·x\n",
        "\n",
        "in the lecture you defined the loss with log2(·), but for optimization purposes the logarithm base doesn’t matter. Derive the gradient update for this case, and implement\n",
        "the appropriate SGD function"
      ],
      "metadata": {
        "id": "gKncJmAGrBdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "snNCckAOp7r6"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "import scipy.special\n",
        "from sklearn.datasets import fetch_openml\n",
        "import sklearn.preprocessing\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The helper function reads the examples labelled 0, 8 and returns them with the labels −1/+1."
      ],
      "metadata": {
        "id": "A4C5pHkusbMJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def helper():\n",
        "    mnist = fetch_openml('mnist_784', as_frame=False)\n",
        "    data = mnist['data']\n",
        "    labels = mnist['target']\n",
        "\n",
        "    neg, pos = \"0\", \"8\"\n",
        "    train_idx = numpy.random.RandomState(0).permutation(np.where((labels[:60000] == neg) | (labels[:60000] == pos))[0])\n",
        "    test_idx = numpy.random.RandomState(0).permutation(np.where((labels[60000:] == neg) | (labels[60000:] == pos))[0])\n",
        "\n",
        "    train_data_unscaled = data[train_idx[:6000], :].astype(float)\n",
        "    train_labels = (labels[train_idx[:6000]] == pos) * 2 - 1\n",
        "\n",
        "    validation_data_unscaled = data[train_idx[6000:], :].astype(float)\n",
        "    validation_labels = (labels[train_idx[6000:]] == pos) * 2 - 1\n",
        "\n",
        "    test_data_unscaled = data[60000 + test_idx, :].astype(float)\n",
        "    test_labels = (labels[60000 + test_idx] == pos) * 2 - 1\n",
        "\n",
        "    # Preprocessing\n",
        "    train_data = sklearn.preprocessing.scale(train_data_unscaled, axis=0, with_std=False)\n",
        "    validation_data = sklearn.preprocessing.scale(validation_data_unscaled, axis=0, with_std=False)\n",
        "    test_data = sklearn.preprocessing.scale(test_data_unscaled, axis=0, with_std=False)\n",
        "    return train_data, train_labels, validation_data, validation_labels, test_data, test_labels\n",
        "\n",
        "\n",
        "def SGD_hinge(data, labels, C, eta_0, T):\n",
        "    \"\"\"\n",
        "    Implements SGD for hinge loss.\n",
        "    \"\"\"\n",
        "    w = np.array([0 for i in range(len(data[0]))])\n",
        "    for t in range(1, T + 1):\n",
        "        eta = eta_0 / t\n",
        "        i = random.randint(0, len(data) - 1)\n",
        "        if np.dot(labels[i] * w, data[i]) < 1:\n",
        "            w = (1 - eta) * w + eta * C * labels[i] * data[i]\n",
        "        else:\n",
        "            w = (1 - eta) * w\n",
        "    return w\n",
        "\n",
        "def SGD_log(data, labels, eta_0, T):\n",
        "    \"\"\"\n",
        "    Implements SGD for log loss.\n",
        "    \"\"\"\n",
        "    w = np.array([0 for i in range(len(data[0]))])\n",
        "    for t in range(1, T + 1):\n",
        "        eta = eta_0 / t\n",
        "        i = random.randint(0, len(data) - 1)\n",
        "        w = w - eta * gradient(w, data[i], labels[i])\n",
        "    return w\n"
      ],
      "metadata": {
        "id": "017Jy4DqqQXH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will continue working with the MNIST data\n",
        "set. The file template (skeleton sgd.py), contains the code to load the training,\n",
        "validation and test sets for the digits 0 and 8 from the MNIST data. \n",
        "\n",
        "In this exercise\n",
        "we will optimize the Hinge loss with L2-regularization \n",
        "\n",
        "(ℓ(w, x, y) = C(max{0, 1 −\n",
        "y⟨w, x⟩}) + 0.5∥w∥\n",
        "2\n",
        "),\n",
        "\n",
        " using the stochastic gradient descent implementation discussed\n",
        "in class. \n",
        "\n",
        "Namely, we initialize w1 = 0, and at each iteration t = 1, . . . we sample i\n",
        "uniformly; and if yiwt\n",
        "· xi < 1, we update:\n",
        "\n",
        "wt+1 = (1 − ηt)wt + ηtCyixi\n",
        "\n",
        "and wt+1 = (1 − ηt)wt otherwise, where ηt = η0/t, and η0 is a constant. \n",
        "\n",
        "Implement an SGD function that accepts the samples and their labels, C, η0 and T, and runs\n",
        "T gradient updates as specified above. In the questions that follow, make sure your\n",
        "graphs are meaningful. Consider using set xlim or set ylim to concentrate only on\n",
        "a relevant range of values."
      ],
      "metadata": {
        "id": "gFVn2cR1sn_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#################################\n",
        "# Place for additional code\n",
        "\n",
        "def gradient(w, x, y):\n",
        "    z = y * np.dot(w, x)\n",
        "    return -y * x / (1 + np.exp(z))\n",
        "\n",
        "\n",
        "def SGD(data, w):\n",
        "    labels = []\n",
        "    for x in data:\n",
        "        if np.dot(x, w) > 0:\n",
        "            labels.append(1)\n",
        "        else:\n",
        "            labels.append(-1)\n",
        "    return labels\n",
        "\n",
        "\n",
        "def diff_rate(y1, y2):\n",
        "    count = 0\n",
        "    for i in range(len(y1)):\n",
        "        if y1[i] == y2[i]:\n",
        "            count += 1\n",
        "    return count / len(y1)\n",
        "\n",
        "\n",
        "def ques1_a(data, labels, cross_data, cross_labels, T, C):\n",
        "    # etas = [math.pow(10, i) for i in range(-5, 6)]\n",
        "    etas = [i / 100 for i in range(70, 80)]\n",
        "    average = []\n",
        "    for eta_0 in etas:\n",
        "        count = 0\n",
        "        for _ in range(10):\n",
        "            w = SGD_hinge(data, labels, C, eta_0, T)\n",
        "            exam_labels = SGD(cross_data, w)\n",
        "            count += diff_rate(exam_labels, cross_labels)\n",
        "        average.append(count / 10)\n",
        "    plt.plot(etas, average)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def ques1_b(data, labels, cross_data, cross_labels, T, eta_0):\n",
        "    cis = [i for i in range(-5, 6)]\n",
        "    average = []\n",
        "    for C in cis:\n",
        "        count = 0\n",
        "        for _ in range(10):\n",
        "            w = SGD_hinge(data, labels, math.pow(10, C), eta_0, T)\n",
        "            exam_labels = SGD(cross_data, w)\n",
        "            count += diff_rate(exam_labels, cross_labels)\n",
        "        average.append(count / 10)\n",
        "    plt.plot(cis, average)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def ques1_c(data, labels, eta0, C, T):\n",
        "    w = SGD_hinge(data, labels, C, eta0, T)\n",
        "    plt.imshow(np.reshape(w, (28, 28)), interpolation=\"nearest\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def ques1_d(data, labels, test_data, test_labels, eta0, C, T):\n",
        "    w = SGD_hinge(data, labels, C, eta0, T)\n",
        "    new_l = SGD(test_data, w)\n",
        "    return diff_rate(new_l, test_labels)\n",
        "\n",
        "\n",
        "def ques2_a(data, labels, cross_data, cross_labels, T):\n",
        "    etas = [i / 100000 for i in range(1, 10)]\n",
        "    # etas = [i / 100 for i in range(70, 80)]\n",
        "    average = []\n",
        "    for eta_0 in etas:\n",
        "        count = 0\n",
        "        for _ in range(10):\n",
        "            # w = SGD_log(data, labels, math.pow(10, eta_0), T)\n",
        "            w = SGD_log(data, labels, eta_0, T)\n",
        "            exam_labels = SGD(cross_data, w)\n",
        "            count += diff_rate(exam_labels, cross_labels)\n",
        "        average.append(count / 10)\n",
        "    plt.plot(etas, average)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def ques2_b(data, labels, eta0, test_data, test_labels, T):\n",
        "    w = SGD_log(data, labels, eta0, T)\n",
        "    plt.imshow(np.reshape(w, (28, 28)), interpolation=\"nearest\")\n",
        "    plt.show()\n",
        "    new_l = SGD(test_data, w)\n",
        "    return diff_rate(new_l, test_labels)\n",
        "\n",
        "\n",
        "def ques2_c(data, labels, T, eta_0):\n",
        "    norm = []\n",
        "    T_arr = [i for i in range(T)]\n",
        "    w = np.array([0 for i in range(len(data[0]))])\n",
        "    for t in T_arr:\n",
        "        eta = eta_0 / (t + 1)\n",
        "        i = random.randint(0, len(data) - 1)\n",
        "        w = w - eta * gradient(w, data[i], labels[i])\n",
        "        norm.append(np.linalg.norm(w))\n",
        "    plt.plot(T_arr, norm)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "zXMTM5SrqQZe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# question 1\n",
        "(a)  Train the classifier on the training set. Use cross-validation on the\n",
        "validation set to find the best η0, assuming T = 1000 and C = 1. For each possible\n",
        "η0 (for example, you can search on the log scale η0 = 10^−5\n",
        ", 10^−4\n",
        ", . . . , 10^4\n",
        ", 10^5\n",
        "and increase resolution if needed), assess the performance of η0 by averaging the\n",
        "accuracy on the validation set across 10 runs. Plot the average accuracy on the\n",
        "validation set, as a function of η0.\n",
        "\n",
        "(b)  Now, cross-validate on the validation set to find the best C given the\n",
        "best η0 you found above. For each possible C (again, you can search on the log\n",
        "scale as in section (a)), average the accuracy on the validation set across 10 runs.\n",
        "Plot the average accuracy on the validation set, as a function of C.\n",
        "\n",
        "(c) Using the best C, η0 you found, train the classifier, but for T = 20000.\n",
        "Show the resulting w as an image, e.g. using the following matplotlib.pyplot\n",
        "function: imshow(reshape(image, (28, 28)), interpolation=’nearest’).\n",
        "Give an intuitive interpretation of the image you obtain.\n",
        "\n",
        "(d) What is the accuracy of the best classifier on the test set?\n",
        "\n",
        "# question 2\n",
        "\n",
        "(a)  Train the classifier on the training set. Use cross-validation on the\n",
        "validation set to find the best η0, assuming T = 1000. For each possible η0 (for\n",
        "example, you can search on the log scale η0 = 10^−5\n",
        ", 10^−4\n",
        ", . . . , 10^4\n",
        ", 10^5 and increase\n",
        "resolution if needed), assess the performance of η0 by averaging the accuracy on\n",
        "the validation set across 10 runs. Plot the average accuracy on the validation set,\n",
        "as a function of η0.\n",
        "\n",
        "(b)  Using the best η0 you found, train the classifier, but for T = 20000.\n",
        "Show the resulting w as an image. What is the accuracy of the best classifier on\n",
        "the test set?\n",
        "\n",
        "(c)  Train the classifier for T = 20000 iterations, and plot the norm of\n",
        "w as a function of the iteration. How does the norm change as SGD progresses?\n"
      ],
      "metadata": {
        "id": "Fj9buwD1rlY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels, validation_data, validation_labels, test_data, test_labels = helper()\n",
        "# ques1_a(train_data, train_labels, validation_data, validation_labels, 1000, 1)\n",
        "# ques1_b(train_data, train_labels, validation_data, validation_labels, 1000, 0.725)\n",
        "# ques1_c(train_data, train_labels, 0.725, math.pow(10, -4), 20000)\n",
        "# print(ques1_d(train_data, train_labels, test_data, test_labels, 0.725, math.pow(10, -4), 20000))\n",
        "# ques2_a(train_data, train_labels, validation_data, validation_labels, 1000)\n",
        "print(ques2_b(train_data, train_labels, math.pow(10, -5), test_data, test_labels, 20000))\n",
        "#ques2_c(train_data, train_labels, 20000, math.pow(10, -5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "HIZhlDk7qQbw",
        "outputId": "6f9e0ce3-05ae-46c3-94df-fd26462811ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9728761514841351\n"
          ]
        }
      ]
    }
  ]
}